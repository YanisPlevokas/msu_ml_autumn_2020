{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool, Queue\n",
    "import lxml\n",
    "import tldextract\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import GermanStemmer, EnglishStemmer, RussianStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_symbols = re.compile(r'[^a-zA-Z0-9а-яА-Я\\u00E4\\u00F6\\u00FC\\u00C4\\u00D6\\u00DC\\u00df]')\n",
    "stopwords_new = ['http', 'url', 'img','html', 'https', 'org', 'www', 'jpg', 'png', 'net','com','php', 'uid','src', 'ahttp', 'index', 'htm']\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords_new) + r')\\b\\s*')\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "musor = re.compile(r'\\d{1}(\\w{2})\\d{2,4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_ru = RussianStemmer()\n",
    "stemmer_eng = EnglishStemmer()\n",
    "stemmer_ger = GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(['english', 'russian', 'german']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_position_start = 1\n",
    "my_position_end = 18685\n",
    "queue = Queue() # очередь ссылок на книги\n",
    "for i in range(my_position_start, my_position_end):\n",
    "    queue.put(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split title to words\n",
    "def split_title(title):\n",
    "    words = nltk.word_tokenize(title)\n",
    "    without_extra_words = [stemmer_ru.stem(stemmer_eng.stem(stemmer_ger.stem(word))) for word in words if word not in stop_words]\n",
    "    return without_extra_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split text to words\n",
    "def split_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    without_extra_words = [stemmer_ru.stem(stemmer_eng.stem(stemmer_ger.stem(word))) for word in words if word not in stop_words]\n",
    "    return without_extra_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns words which title contains and text contains\n",
    "def get_content(title, text, n=None):\n",
    "    title_words = split_title(title)\n",
    "    text_words = split_text(text)\n",
    "    if n is not None:\n",
    "        vectorizer = CountVectorizer().fit(text_words)\n",
    "        tmp_words_matrix = vectorizer.transform(text_words)\n",
    "        tmp_words_count = np.sum(tmp_words_matrix, axis=0)\n",
    "        tmp_words = [(word, tmp_words_count[0, ind]) for word, ind in vectorizer.vocabulary_.items()]\n",
    "        tmp_words = sorted(tmp_words, reverse=True, key=lambda x: x[1])\n",
    "        tmp_words = [word[0] for word in tmp_words[:n]]\n",
    "        doc_words = title_words + tmp_words\n",
    "    else:\n",
    "        doc_words = title_words + text_words\n",
    "    return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(number_of_doc):\n",
    "    with open('./content/{:d}.dat'.format(number_of_doc), encoding='utf-8') as file:\n",
    "        text_checker = {}\n",
    "        html_test = file.read()\n",
    "        soup = BeautifulSoup(html_test, 'html.parser')\n",
    "        if soup.title:\n",
    "            title_name = soup.title.text\n",
    "            title_name = regex_symbols.sub(\" \", title_name)\n",
    "            title_name = re.sub(\"\\s\\s+\" , \" \", title_name)\n",
    "        else:\n",
    "            title_name = ' '\n",
    "        url = soup.text[:soup.text.index('\\n')]\n",
    "        url = tldextract.extract(url)\n",
    "        url = url.domain + '.' + url.suffix\n",
    "        text = soup.text[soup.text.index('\\n'):].lower()\n",
    "        text = regex_symbols.sub(\" \", text)\n",
    "        text = shortword.sub(\" \", text)\n",
    "        text = pattern.sub(\" \", text)\n",
    "        text = musor.sub(' ', text)\n",
    "        text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "        text = re.sub('\\xa0|\\xad', ' ', text)\n",
    "        print(text)\n",
    "        content = get_content(title_name, text, n=25)\n",
    "        text_checker[number_of_doc] = [url] + content\n",
    "        return text_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_doc(25838)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 765/9342 [14:58<3:39:55,  1.54s/it] 10114\n",
      "'NoneType' object has no attribute 'text'\n",
      " 50%|█████     | 4681/9342 [1:36:40<1:36:11,  1.24s/it]14030\n",
      "'NoneType' object has no attribute 'text'\n",
      " 53%|█████▎    | 4934/9342 [1:41:12<2:27:30,  2.01s/it]"
     ]
    }
   ],
   "source": [
    "def process_all_docs(i):\n",
    "    with gzip.open('data/part_{:05d}.jsonl.gz'.format(i), mode='wb') as f_json:\n",
    "        f_json = codecs.getwriter('utf8')(f_json)\n",
    "\n",
    "        while not queue.empty():\n",
    "            try:\n",
    "                id_new = queue.get()\n",
    "                record = process_doc(id_new)\n",
    "            except Exception as e:\n",
    "                print(id_new, file=sys.stderr)\n",
    "                print(e, file=sys.stderr)\n",
    "            record_str = json.dumps(record, ensure_ascii=False)\n",
    "            print(record_str, file=f_json)\n",
    "\n",
    "            # счетчик должен атомарно обновиться\n",
    "            with lock:\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "with Pool(processes=8) as pool, tqdm(total=queue.qsize()) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    pool.map(process_all_docs, range(pool._processes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
